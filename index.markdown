---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

title: "Audio Imagination: NeurIPS 2024 Workshop on AI-Driven Speech, Music, and Sound Generation"
layout: default
---

# Abstract

Generative AI has been at the forefront of AI research in the most recent times. A large number of research works have shown remarkable and, to some extent, surprising generation capabilities, and across different modalities (e.g., text, image, and audio). ChatGPT, Gemini, DALL-E, Stable Diffusion, Imagen, and Make-A-Video are just a few examples where generative AI has captivated attention of the researchers as well as the general public.  

Audio generation presents unique challenges given the nature of audio signal. The perception of audio is largely driven by its frequency characteristics over time. Moreover, sounds play an important role in providing spatial sense to humans and for generation of perceptually realistic audio, several environmental factors needs to be accounted for.  In addition, two or more overlapping acoustic phenomena can each be saliently perceived by humans, unlike visual objects, which could occlude each other. Hence, AI approaches need to account for these characteristics of sounds while learning to generate audio. Moreover, human perception of sounds is spatialized and hence spatial audio generation is crucial for producing real-life like experiences through generative audio technologies. Other factors such room acoustics, environments, and visual synchronization also play a critical role in the perception of sounds which further makes audio generation research unique and challenging. 

Some audio generation tasks such as text-to-speech synthesis, voice conversion, and speech enhancement/separation have been studied for a while in the speech/audio/music research community. The strong capabilities of modern generative modeling bring numerous new opportunities towards solving these old problems (for example by modeling audio signals as sequences of discrete tokens similar to word tokens).

At the same time, there are several new problems to study. For example, in the most recent times, generating music and sounds from natural language inputs has gained significant attention and several research papers have come out. Another is generating audiovisual content which has the potential to make a significant impact on the future of media creation. 


This workshop aims to bring together researchers working on different audio generation problems and enable concentrated discussions on the topic. The workshop will feature exciting invited talks, high-quality papers presented through oral and poster sessions and a demo session to let everyone experience the current state of audio generation methods. 

# Speakers

<table style="border-collapse: collapse; border: none;">
<tr style="border: none;">
    <td style="border: none;">
        <img style="width: auto; height: auto;  max-height:128px;  max-width:128px; position: relative; overflow: hidden; border-radius: 50%;" 
        src="./assets/img/alexis_conneau.jpeg">
    </td>
    <td style="border: none;"><a href="https://scholar.google.fr/citations?user=45KfCpgAAAAJ">Alexis Conneau</a></td>
    <td style="border: none;">OpenAI, USA</td>
</tr>
<tr style="border: none;">
    <td style="border: none;">
        <img style="width: auto; height: auto;  max-height:128px;  max-width:128px; position: relative; overflow: hidden; border-radius: 50%;" 
        src="./assets/img/yao_xie.jpg">
    </td>
    <td style="border: none;"><a href="https://www2.isye.gatech.edu/~yxie77/">Yao Xie</a></td>
    <td style="border: none;">Georgia Institute of Technology, USA</td>
</tr>
<tr style="border: none;">
    <td style="border: none;">
        <img style="width: auto; height: auto;  max-height:128px;  max-width:128px; position: relative; overflow: hidden; border-radius: 50%;" 
        src="./assets/img/zhou_zhao.jpg">
    </td>
    <td style="border: none;"><a href="https://mypage.zju.edu.cn/zhaozhou">Zhou Zhao</a></td>
    <td style="border: none;">Zhejiang University, China</td>
</tr>
<tr style="border: none;">
    <td style="border: none;">
        <img style="width: auto; height: auto;  max-height:128px;  max-width:128px; position: relative; overflow: hidden; border-radius: 50%;" 
        src="./assets/img/anna_huang.jpg">
    </td>
    <td style="border: none;"><a href="https://czhuang.github.io/">Anna Huang</a></td>
    <td style="border: none;">Massachusetts Institute of Technology, USA</td>
</tr>
<tr style="border: none;">
    <td style="border: none;">
        <img style="width: auto; height: auto;  max-height:128px;  max-width:128px; position: relative; overflow: hidden; border-radius: 50%;" 
        src="./assets/img/joon_soo.jpeg">
    </td>
    <td style="border: none;"><a href="https://mm.kaist.ac.kr/joon/">Joon Son Chung</a></td>
    <td style="border: none;">Korea Advanced Institute of Science & Technology (KAIST), Korea</td>
</tr>
<tr style="border: none;">
    <td style="border: none;">
        <img style="width: auto; height: auto;  max-height:128px;  max-width:128px; position: relative; overflow: hidden; border-radius: 50%;" 
        src="./assets/img/vikas_chandra.jpg">
    </td>
    <td style="border: none;"><a href="https://v-chandra.github.io/">Vikas Chandra</a></td>
    <td style="border: none;">Meta, USA</td>
</tr>
</table>

# Programme

| Time  | Schedule |
| :--   | :--  |
| 08:00 - 08:15 | Welcome and opening remarks |
| 08:15 - 08:45 | Alexis Conneau's talk |
| 08:45 - 09:15 | Yao Xie's talk |
| 09:15 - 09:45 | Zhou Zhao's talk |
| 09:45 - 10:30 | Oral paper presentations |
| 10:30 - 12:00 | Poster + Demos Session |
| 12:00 - 13:00 | Break |
| 13:00 - 13:30 | Anna Huang's talk |
| 13:30 - 14:00 | Joon Son Chung's talk |
| 14:00 - 14:30 | Vikas Chandra's talk |
| 14:30 - 15:15 | Oral paper presentations |
| 15:15 - 15:30 | Short Break |
| 15:30 - 17:00 | Poster and Demo presentations |
| 17:00 - 17:45 | Panel Discussion and Closing Remarks |

# Paper Submission
## Main paper track
We will invite researchers to submit papers focusing on, but not limited to, the following topics related to audio generation:

- Textual prompts and natural language inputs based generation and editing of audio, such as text-to-speech (i.e., speech synthesis), and text-to-music and text-to-sound, where text prompts are used to describe what signals to generate.
- Multimodal generation of audio - going beyond text to visual, acoustic and other forms of inputs. Generating audio that is synchronized with the video.
- Relationship with and impact of generative AI techniques on established speech/audio/music tasks where audio is generated such as speech enhancement, source separation, voice conversion, speech to speech translation, to mention a few. 
- Generation of spatial audio and experiences driven by spatial audio.
- Generation of audio for virtual or augmented reality (VR/AR), where, for example, spatial awareness of sound objects needs to be realistic, and generated signals need to be aligned with other modalities such as visual cues.
- Impact of generative audio on content creators - we aim to invite content creators to share their experiences and insights on how generative AI models could impact their productivity and unlock their creativity.
- Interpretability in generative AI for audio/speech/music.
- Responsibility in generative AI for audio/speech/music.
- Novel applications of generative AI in audio/speech/music.
- Connection of audio generation with language generation, including similarities and differences.

## Demo Session
A key component of the proposed workshop is that we will hold an onsite demo session, where participants can have a unique chance to showcase their advanced audio, speech, and music generation technologies, in addition to sharing their academic findings through talks and posters.
- Authors of accepted papers will be encouraged to also have a demo of their generation method during their paper presentation
- Researchers can submite a 3-page submission outlining details of their demo to a separate demo track.

## Submission link: coming soon.

# Dates

TBD - Paper submision ddl

TBD - Paper notification

14th December 2024 - Workshop

# Organizers

<table style="border-collapse: collapse; border: none;">
<tr style="border: none;">
    <td style="border: none;">
        <img style="width: auto; height: auto;  max-height:128px;  max-width:128px; position: relative; overflow: hidden; border-radius: 50%;" 
        src="./assets/img/anurag_kumar.jpg">
    </td>
    <td style="border: none;"><a href="https://anuragkr90.github.io">Anurag Kumar</a></td>
    <td style="border: none;">Research Lead and Scientist at Meta, USA</td>
</tr>
<tr style="border: none;">
    <td style="border: none;">
        <img style="width: auto; height: auto;  max-height:128px;  max-width:128px; position: relative; overflow: hidden; border-radius: 50%;" 
        src="./assets/img/zhaoheng_ni.jpg">
    </td>
    <td style="border: none;"><a href="https://nateanl.github.io/">Zhaoheng Ni</a></td>
    <td style="border: none;">Research Scientist at Meta, USA</td>
</tr>
<tr style="border: none;">
    <td style="border: none;">
        <img style="width: auto; height: auto;  max-height:128px;  max-width:128px; position: relative; overflow: hidden; border-radius: 50%;" 
        src="./assets/img/yapeng_tian.jpg">
    </td>
    <td style="border: none;"><a href="https://www.yapengtian.com/">Yapeng Tian</a></td>
    <td style="border: none;">Assistant Professor at The University of Texas at Dallas, USA</td>
</tr>
<tr style="border: none;">
    <td style="border: none;">
        <img style="width: auto; height: auto;  max-height:128px;  max-width:128px; position: relative; overflow: hidden; border-radius: 50%;" 
        src="./assets/img/berrek_sisman.png">
    </td>
    <td style="border: none;"><a href="https://ece.utdallas.edu/staff/sisman/">Berrak Sisman</a></td>
    <td style="border: none;">Assistant professor at The University of Texas at Dallas, USA</td>
</tr>
<tr style="border: none;">
    <td style="border: none;">
        <img style="width: auto; height: auto;  max-height:128px;  max-width:128px; position: relative; overflow: hidden; border-radius: 50%;" 
        src="./assets/img/shinji_watanabe.jpg">
    </td>
    <td style="border: none;"><a href="https://sites.google.com/view/shinjiwatanabe">Shinji Watanabe</a></td>
    <td style="border: none;">Associate Professor at Carnegie Mellon University, USA</td>
</tr>
<tr style="border: none;">
    <td style="border: none;">
        <img style="width: auto; height: auto;  max-height:128px;  max-width:128px; position: relative; overflow: hidden; border-radius: 50%;" 
        src="./assets/img/wenwu_wang.webp">
    </td>
    <td style="border: none;"><a href="https://www.surrey.ac.uk/people/wenwu-wang">Wenwu Wang</a></td>
    <td style="border: none;">Professor at University of Surrey, United Kingdom</td>
</tr>
</table>
